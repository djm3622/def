{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce169423-3e3a-44f0-b38d-cb69e8aa6bf4",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Notebook for distriputed training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013afb49-8586-42ba-baca-e639baf1d16f",
   "metadata": {},
   "source": [
    "# Imports/Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79ece1a5-285a-4c11-861f-a6844f3ddd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 18:12:56.451745: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-27 18:12:56.466336: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-27 18:12:56.485147: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-27 18:12:56.490925: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-27 18:12:56.504446: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-27 18:12:57.405383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator, notebook_launcher\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wandb_helper import init_wandb, save_model_architecture, finish_run\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from safetensors.torch import load_file\n",
    "from diffusers import UNet2DModel\n",
    "import data\n",
    "import dataset\n",
    "import model\n",
    "import conditional\n",
    "import math\n",
    "import utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53f52713-7c77-4e60-900c-acd9ee1bdf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:    \n",
    "    # dataset\n",
    "    path = '/data/users/jupyter-dam724/colliding_solutions'\n",
    "    solver = 'ros2'\n",
    "    fixed_seq_len = 216\n",
    "    ahead = 1\n",
    "    tail = 1\n",
    "    aug = True\n",
    "    upsample_size = 96\n",
    "\n",
    "    # device (not used but needed for dataset)\n",
    "    device_pref = 'cuda'\n",
    "    device_ind = None\n",
    "    \n",
    "    # distributed training\n",
    "    num_processes = 3\n",
    "    per_gpu_batch_size = 3\n",
    "    total_batch_size = per_gpu_batch_size * num_processes # (temporarily removed)\n",
    "    workers_per_gpu = 6\n",
    "    tworkers = workers_per_gpu * num_processes\n",
    "    vworkers = workers_per_gpu * num_processes\n",
    "    grad_accumulate = 8\n",
    "    \n",
    "    # optimization\n",
    "    base_lr = 1e-5\n",
    "    max_lr = 1e-4\n",
    "    lr = base_lr * math.sqrt(total_batch_size / (per_gpu_batch_size))  # sqrt scaling\n",
    "    \n",
    "    # training\n",
    "    epoches = 100\n",
    "    timesteps = 4000\n",
    "    loss_type = \"simple\"\n",
    "    sample_delay = 10\n",
    "    \n",
    "    # experimentations\n",
    "    project_name = \"Operator Guided Diffusion\"\n",
    "    experiment_name = 'init-conditional-opout-resumed'\n",
    "    save_path = f'/data/users/jupyter-dam724/time-invariant-operator/checkpoint/{experiment_name}/'\n",
    "    utility.validate_and_create_save_path(save_path, experiment_name)\n",
    "    from_checkpoint = None \n",
    "    op_ckpt = '/data/users/jupyter-dam724/time-invariant-operator/checkpoint/operator-training-adjusted-dropoutbigger/valid/model.safetensors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3860894-725c-43a6-903b-bf3082685b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavid724\u001b[0m (\u001b[33mdavid724-lehigh-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/users/jupyter-dam724/time-invariant-operator/wandb/run-20250127_181300-mqi7izyy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/david724-lehigh-university/Operator%20Guided%20Diffusion/runs/mqi7izyy' target=\"_blank\">init-conditional-opout-restart</a></strong> to <a href='https://wandb.ai/david724-lehigh-university/Operator%20Guided%20Diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/david724-lehigh-university/Operator%20Guided%20Diffusion' target=\"_blank\">https://wandb.ai/david724-lehigh-university/Operator%20Guided%20Diffusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/david724-lehigh-university/Operator%20Guided%20Diffusion/runs/mqi7izyy' target=\"_blank\">https://wandb.ai/david724-lehigh-university/Operator%20Guided%20Diffusion/runs/mqi7izyy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    }
   ],
   "source": [
    "init_wandb(\n",
    "    project_name=Config.project_name,\n",
    "    run_name=Config.experiment_name,\n",
    "    config_class=Config,\n",
    "    save_path=Config.save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f831c777-4f2e-4d41-b9fb-d991260fe134",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab68e8-39f1-496a-b2e5-514c235289c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : move to model file after works\n",
    "\n",
    "def load_training_state(accelerator, checkpoint_path, model, optimizer, scheduler):\n",
    "    # Load state dict\n",
    "    state = torch.load(checkpoint_path, map_location=accelerator.device)\n",
    "    \n",
    "    # Restore model state\n",
    "    accelerator.unwrap_model(model).load_state_dict(state['model_state_dict'])\n",
    "    \n",
    "    # Restore optimizer state\n",
    "    optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "    \n",
    "    # Restore scheduler if it exists\n",
    "    if scheduler is not None and state['scheduler_state_dict'] is not None:\n",
    "        scheduler.load_state_dict(state['scheduler_state_dict'])\n",
    "    \n",
    "    # Restore RNG states\n",
    "    rng_states = state['rng_states']\n",
    "    random.setstate(rng_states['python'])\n",
    "    np.random.set_state(rng_states['numpy'])\n",
    "    torch.set_rng_state(rng_states['torch'])\n",
    "    if torch.cuda.is_available() and rng_states['cuda'] is not None:\n",
    "        torch.cuda.set_rng_state_all(rng_states['cuda'])\n",
    "    \n",
    "    return state['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868124c4-2043-4813-a2b0-6e9c2131b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acelerate_ddp():\n",
    "    accelerator = Accelerator(gradient_accumulation_steps=Config.grad_accumulate)\n",
    "    \n",
    "    data_params = {\n",
    "        'path': Config.path, \n",
    "        'device_pref': Config.device_pref, \n",
    "        'solver': Config.solver, \n",
    "        'fixed_seq_len': Config.fixed_seq_len, \n",
    "        'ahead': Config.ahead, \n",
    "        'tail': Config.tail,\n",
    "        'device_ind': Config.device_ind\n",
    "    }\n",
    "\n",
    "    _, (x_train_data, y_train_data), (x_valid_data, y_valid_data) = data.main(**data_params)\n",
    "    \n",
    "    dataset_params = {\n",
    "        'x_train_data': x_train_data, \n",
    "        'x_valid_data': x_valid_data,\n",
    "        'batch_size': Config.total_batch_size,\n",
    "        't_timesteps': Config.timesteps,\n",
    "        'tworkers': Config.tworkers, \n",
    "        'vworkers': Config.vworkers,\n",
    "        'upsample_size': Config.upsample_size,\n",
    "        'aug': Config.aug\n",
    "    }\n",
    "\n",
    "    train_dl, valid_dl = dataset.main(**dataset_params)\n",
    "    \n",
    "    unet = UNet2DModel(\n",
    "        sample_size=(Config.upsample_size, Config.upsample_size),        \n",
    "        in_channels=2,         \n",
    "        out_channels=1,         \n",
    "        layers_per_block=4,      \n",
    "        block_out_channels=(64, 128, 256, 512),  \n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",      # 128 channels at 96x96\n",
    "            \"DownBlock2D\",      # 256 channels at 48x48\n",
    "            \"AttnDownBlock2D\",  # 384 channels at 24x24\n",
    "            \"AttnDownBlock2D\",  # 512 channels at 12x12\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"AttnUpBlock2D\",\n",
    "            \"AttnUpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    save_model_architecture(unet, Config.save_path)\n",
    "    \n",
    "    # TODO : load from state and continue training\n",
    "    \n",
    "    if Config.from_checkpoint is not None:\n",
    "        state_dict = load_file(Config.from_checkpoint)\n",
    "        model.load_model_weights(unet, state_dict)\n",
    "        \n",
    "    operator = UNet2DModel(\n",
    "        sample_size=(Config.upsample_size, Config.upsample_size),        \n",
    "        in_channels=1,         \n",
    "        out_channels=1,         \n",
    "        layers_per_block=2,      \n",
    "        block_out_channels=(64, 64, 128, 64),  \n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",      # 64 channels at 96x96\n",
    "            \"DownBlock2D\",      # 64 channels at 48x48\n",
    "            \"AttnDownBlock2D\",  # 128 channels at 24x24\n",
    "            \"AttnDownBlock2D\"   # 64 channels at 12x12\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"AttnUpBlock2D\",\n",
    "            \"AttnUpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if Config.op_ckpt is not None:\n",
    "        state_dict = load_file(Config.op_ckpt)\n",
    "        model.load_model_weights(operator, state_dict)\n",
    "\n",
    "    optimizer = optim.AdamW(unet.parameters(), lr=Config.lr)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=Config.max_lr,\n",
    "        epochs=Config.epoches,\n",
    "        steps_per_epoch=len(train_dl),\n",
    "        pct_start=0.1,  \n",
    "        div_factor=25,  \n",
    "        final_div_factor=1e4 \n",
    "    )\n",
    "    \n",
    "    # Send everything through `accelerator.prepare`\n",
    "    train_dl, valid_dl, unet, operator, optimizer, scheduler = accelerator.prepare(\n",
    "        train_dl, valid_dl, unet, operator, optimizer, scheduler\n",
    "    )\n",
    "        \n",
    "    train_log, valid_log = [], []\n",
    "    \n",
    "    training_params = {\n",
    "        'accelerator': accelerator,\n",
    "        'train': train_dl, \n",
    "        'valid': valid_dl,\n",
    "        'model': unet, \n",
    "        'operator': operator,\n",
    "        'epochs': Config.epoches, \n",
    "        'criterion': nn.MSELoss(), \n",
    "        'save_path': Config.save_path, \n",
    "        'loss_type': Config.loss_type,\n",
    "        'train_log': train_log, \n",
    "        'optimizer': optimizer, \n",
    "        'scheduler': scheduler, \n",
    "        'sample_delay': Config.sample_delay,\n",
    "        't_timesteps': Config.timesteps,\n",
    "        'size': Config.upsample_size,\n",
    "        'loading_bar': False\n",
    "    }\n",
    "    \n",
    "    conditional.accelerator_train(**training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a92728c-f611-4fdc-a233-21716cd4c8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 3 GPUs.\n",
      "Now using GPU.\n",
      "Now using GPU.\n",
      "Now using GPU.\n",
      "Train size: 145097, Percent of toal: 74.68%, Unique instances: 700\n",
      "Train size: 49194, Percent of toal: 25.32%, Unique instances: 240\n",
      "Train size: 145097, Percent of toal: 74.68%, Unique instances: 700\n",
      "Train size: 49194, Percent of toal: 25.32%, Unique instances: 240\n",
      "Train size: 145097, Percent of toal: 74.68%, Unique instances: 700\n",
      "Train size: 49194, Percent of toal: 25.32%, Unique instances: 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/users/jupyter-dam724/.local/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [256, 512, 1, 1], strides() = [512, 1, 512, 512]\n",
      "bucket_view.sizes() = [256, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/data/users/jupyter-dam724/.local/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [256, 512, 1, 1], strides() = [512, 1, 512, 512]\n",
      "bucket_view.sizes() = [256, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/data/users/jupyter-dam724/.local/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [256, 512, 1, 1], strides() = [512, 1, 512, 512]\n",
      "bucket_view.sizes() = [256, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100, Train Loss: 0.0019169766455888748\n",
      "Epoch 32/100, Train Loss: 0.0019258473766967654\n",
      "Epoch 33/100, Train Loss: 0.0018703469540923834\n",
      "Epoch 34/100, Train Loss: 0.0018010828644037247\n",
      "Epoch 35/100, Train Loss: 0.0018391864141449332\n",
      "Epoch 36/100, Train Loss: 0.0017801886424422264\n",
      "Epoch 37/100, Train Loss: 0.001698595704510808\n",
      "Epoch 38/100, Train Loss: 0.0016911126440390944\n",
      "Epoch 39/100, Train Loss: 0.001767714275047183\n",
      "Epoch 40/100, Train Loss: 0.0016503711231052876\n",
      "Epoch 41/100, Train Loss: 0.0016521355137228966\n",
      "Epoch 42/100, Train Loss: 0.0016399594023823738\n",
      "Epoch 43/100, Train Loss: 0.0015641842037439346\n",
      "Epoch 44/100, Train Loss: 0.001596556045114994\n",
      "Epoch 45/100, Train Loss: 0.001596342190168798\n",
      "Epoch 46/100, Train Loss: 0.0015544978668913245\n",
      "Epoch 47/100, Train Loss: 0.0015988072846084833\n",
      "Epoch 48/100, Train Loss: 0.0014464225387200713\n",
      "Epoch 49/100, Train Loss: 0.001478452468290925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100, Train Loss: 0.0014201186131685972\n",
      "Epoch 51/100, Train Loss: 0.0014467922737821937\n",
      "Epoch 52/100, Train Loss: 0.0014246515929698944\n",
      "Epoch 53/100, Train Loss: 0.0014211612287908792\n",
      "Epoch 54/100, Train Loss: 0.0014366372488439083\n",
      "Epoch 55/100, Train Loss: 0.00143334676977247\n",
      "Epoch 56/100, Train Loss: 0.0013979028444737196\n",
      "Epoch 57/100, Train Loss: 0.0013845607172697783\n",
      "Epoch 58/100, Train Loss: 0.0014179841382429004\n",
      "Epoch 59/100, Train Loss: 0.001378132845275104\n",
      "Epoch 60/100, Train Loss: 0.0013421426992863417\n",
      "Epoch 61/100, Train Loss: 0.001370538491755724\n",
      "Epoch 62/100, Train Loss: 0.0013494747690856457\n",
      "Epoch 63/100, Train Loss: 0.001302741002291441\n",
      "Epoch 64/100, Train Loss: 0.0013053640723228455\n",
      "Epoch 65/100, Train Loss: 0.001307563274167478\n",
      "Epoch 93/100, Train Loss: 0.0011215182021260262\n",
      "Epoch 94/100, Train Loss: 0.0011180544970557094\n",
      "Epoch 95/100, Train Loss: 0.0011725391959771514\n",
      "Epoch 96/100, Train Loss: 0.0011441658716648817\n",
      "Epoch 97/100, Train Loss: 0.0011163171147927642\n",
      "Epoch 98/100, Train Loss: 0.0011468764860183\n",
      "Epoch 99/100, Train Loss: 0.0011322487844154239\n",
      "Epoch 100/100, Train Loss: 0.0011292481794953346\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(acelerate_ddp, args=(), num_processes=Config.num_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22f12842-7e2d-4308-abb5-90c237840b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8525772cc9476db434cc7b54ee8ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.655 MB of 0.655 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>model_architecture</td><td>UNet2DModel(\n",
       "  (conv...</td></tr><tr><td>step</td><td>99</td></tr><tr><td>train_loss</td><td>0.00113</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">init-conditional-opout-restart</strong> at: <a href='https://wandb.ai/david724-lehigh-university/Operator%20Guided%20Diffusion/runs/mqi7izyy' target=\"_blank\">https://wandb.ai/david724-lehigh-university/Operator%20Guided%20Diffusion/runs/mqi7izyy</a><br/>Synced 6 W&B file(s), 180 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250127_181300-mqi7izyy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finish_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453c27d-4f46-4e0f-88d1-635983e4a426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c85af80-eea7-4dd1-8474-2ea9f94765ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
